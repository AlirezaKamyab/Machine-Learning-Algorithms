{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186ebc2e",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e416844",
   "metadata": {},
   "source": [
    "### Notations\n",
    "Imagine we are to predict the rating for each movie given by a user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b67685",
   "metadata": {},
   "source": [
    "| Notation | Description |\n",
    "| :- | :- |\n",
    "| $n_u$ | Number of users |\n",
    "| $n_m$ | Number of movies |\n",
    "| $r(i, j)$ | It equals 1 if user \"j\" has rated move \"i\" |\n",
    "| $y^{(i, j)}$ | Rating of user \"j\" for the movie \"i\" |\n",
    "| $n$ | Number of features for each movie |\n",
    "| $w^{(j)}$ | Weights of features for user \"j\" |\n",
    "| $x^{(i)}$ | Features for movie \"i\" |\n",
    "| $b^{(j)}$ | Intersection for user \"j\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d7ebc",
   "metadata": {},
   "source": [
    "Given a dataset like below, we can use collaborative filtering to use all the users' rating to predict future ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c2280",
   "metadata": {},
   "source": [
    "| Movie | User1 | User2 | User3 | User4 | User5 | Feature1 ($x_1$) | Feature2 ($x_2$) |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Movie1 | 5 | 5 | 0 | 0 | ? | 0.9 | 0 |\n",
    "| Movie2 | 5 | ? | ? | 0 | ? | 1.0 | 0.01 |\n",
    "| Movie3 | ? | 4 | 0 | ? | ? | 0.99 | 0 |\n",
    "| Movie4 | 0 | 0 | 5 | 4 | ? | 0.1 | 1.0 |\n",
    "| Movie5 | 0 | 0 | 5 | ? | ? | 0 | 0.9 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce35a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518af01e",
   "metadata": {},
   "source": [
    "### Function\n",
    "The function is like a linear function with a difference that we have three variables<br>\n",
    "$\n",
    "f_{W, b, X}(W^{(j)}, b^{(j)}, X^{(i)}) = W^{(j)}.X^{(i)} + b^{(j)}\n",
    "$\n",
    "<br>\n",
    "If the function is mean normalized: <br>\n",
    "$\n",
    "f_{W, b, X}(W^{(j)}, b^{(j)}, X^{(i)}) = W^{(j)}.X^{(i)} + b^{(j)} - \\mu^{(i)}\n",
    "$\n",
    "<br>\n",
    "where $\\mu$ is a vector with $n_m$ rows. (Mean normalization could be calculated in a way to have $n_u$ rows i.e. be calculated via taking the mean of each column rather than row)\n",
    "\n",
    "### Cost function\n",
    "Now instead of learning parameters $W$ and $b$ we also have to learn $X$. <br>\n",
    "$\n",
    "\\begin{equation}\n",
    "\\displaystyle\n",
    "    J(W, b, X) = \\frac{1}{2} \\sum_{(i, j):r(i, j)=1}(W^{(j)}.X^{(i)} + b^{(j)} - y^{(i, j)})^2  + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(W_{k}^{(j)})^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^{n}(X_{k}^{(i)})^2\n",
    "\\end{equation}\n",
    "$\n",
    "<br>\n",
    "We can write this in another way <br>\n",
    "$\n",
    "\\begin{equation}\n",
    "\\displaystyle\n",
    "    J(W, b, X) = \\frac{1}{2} \\sum_{i=1}^{n_m}\\sum_{j=1}^{n_u}r(i, j)\\times(W^{(j)}.X^{(i)} + b^{(j)} - y^{(i, j)})^2  + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(W_{k}^{(j)})^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^{n}(X_{k}^{(i)})^2\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "### Gradient descent\n",
    "Repeat {<br>\n",
    "    $W_{i}^{(j)} = W_{i}^{(j)} - \\alpha\\displaystyle\\frac{\\partial}{\\partial W_{i}^{(j)}}J(W, b, X)$ <br>\n",
    "    $b^{(j)} = b^{(j)} - \\alpha\\displaystyle\\frac{\\partial}{\\partial b^{(j)}}J(W, b, X)$ <br>\n",
    "    $X_{k}^{(j)} = X_{k}^{(j)} - \\alpha\\displaystyle\\frac{\\partial}{\\partial X_{k}^{(j)}}J(W, b, X)$ <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dcd7a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_func(x, w, b):\n",
    "    return np.dot(w, x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958c5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_cost(X, W, b, Y, R, lambda_):\n",
    "    nm, nu = Y.shape\n",
    "    J = 0\n",
    "    nm, n = X.shape\n",
    "    nu, _ = W.shape\n",
    "    \n",
    "    for j in range(nu):\n",
    "        for i in range(nm):\n",
    "            J += R[i, j] * (1/2) * (cf_func(W[j], X[i], b[0, j]) - Y[i, j]) ** 2\n",
    "    \n",
    "    for j in range(nu):\n",
    "        for k in range(n):\n",
    "            J += (lambda_ / 2 ) * W[j, k] ** 2\n",
    "            \n",
    "    for i in range(nm):\n",
    "        for k in range(n):\n",
    "            J += (lambda_ / 2) * X[i, k] ** 2\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751d838",
   "metadata": {},
   "source": [
    "Function below is vectorized implementation of the code (refrence \"Coursera, unsupervised learning, recommenders, reinforcement-learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29639f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func_v(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03fab6",
   "metadata": {},
   "source": [
    "Since finding derivative of the cost function seems difficult, we use \"auto diff\" tools from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dda1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalization(X):\n",
    "    mu = X.sum(axis=1) / X.shape[1]\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    X = X - mu.reshape(-1, 1)\n",
    "    return X, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1f494a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(n, Y, R, alpha=0.1, iterations=200, lambda_=0):\n",
    "    rows, columns = Y.shape\n",
    "    X = tf.Variable(tf.random.normal((rows, n), dtype=tf.dtypes.float64), name='X')\n",
    "    W = tf.Variable(tf.random.normal((columns, n), dtype=tf.dtypes.float64), name='W')\n",
    "    b = tf.Variable(tf.random.normal((1, columns), dtype=tf.dtypes.float64), name='b')\n",
    "    optimizer = tf.keras.optimizers.Adam(alpha)\n",
    "    for i in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost_val = cofi_cost_func_v(X, W, b, Y, R, lambda_)\n",
    "            \n",
    "        grads = tape.gradient(cost_val, [X, W, b])\n",
    "        optimizer.apply_gradients(zip(grads, [X, W, b]))\n",
    "        \n",
    "        print(f'Iteration {i + 1} cost:{cost_val}')\n",
    "    \n",
    "    return X, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ddd041",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3885aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 6\n",
    "nm = 5\n",
    "n = 1\n",
    "\n",
    "# X = np.array([[0.9, 0],\n",
    "#               [0.1, 0.01],\n",
    "#               [0.99, 0],\n",
    "#               [0.1, 1.0],\n",
    "#               [0, 0.9]])\n",
    "\n",
    "X = np.zeros(nm * n).reshape(nm, n)\n",
    "\n",
    "Y = np.array([[5, 5, 0, 0, 4, 0],\n",
    "              [5, 0, 0, 0, 3, 0],\n",
    "              [0, 4, 0, 0, 2, 0],\n",
    "              [0, 0, 5, 4, 4, 0],\n",
    "              [0, 0, 5, 0, 1, 0],])\n",
    "\n",
    "R = np.array([[1, 1, 1, 1, 1, 0],\n",
    "              [1, 0, 0, 1, 1, 0],\n",
    "              [0, 1, 1, 0, 1, 0],\n",
    "              [1, 1, 1, 1, 1, 0],\n",
    "              [1, 1, 1, 0, 1, 0],])\n",
    "\n",
    "b = np.zeros(nu).reshape(1, nu)\n",
    "\n",
    "W = np.zeros(nu * n).reshape(nu, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "33306b83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 cost:49.09495031034808\n",
      "Iteration 2 cost:45.15162116305772\n",
      "Iteration 3 cost:41.39408413041775\n",
      "Iteration 4 cost:37.779166640840515\n",
      "Iteration 5 cost:34.23885108328891\n",
      "Iteration 6 cost:30.71965530968314\n",
      "Iteration 7 cost:27.217707913016852\n",
      "Iteration 8 cost:23.78215830470254\n",
      "Iteration 9 cost:20.462718245745833\n",
      "Iteration 10 cost:17.30776128110569\n",
      "Iteration 11 cost:14.422628350411465\n",
      "Iteration 12 cost:11.954614179742267\n",
      "Iteration 13 cost:10.043465233590112\n",
      "Iteration 14 cost:8.76099566183483\n",
      "Iteration 15 cost:8.043750505402611\n",
      "Iteration 16 cost:7.684702583964462\n",
      "Iteration 17 cost:7.438247094018042\n",
      "Iteration 18 cost:7.133378113317575\n",
      "Iteration 19 cost:6.703201306114898\n",
      "Iteration 20 cost:6.156599224980801\n",
      "Iteration 21 cost:5.538161684477556\n",
      "Iteration 22 cost:4.9044808445010375\n",
      "Iteration 23 cost:4.319345780730934\n",
      "Iteration 24 cost:3.8472537511207756\n",
      "Iteration 25 cost:3.536586016933657\n",
      "Iteration 26 cost:3.403036602252267\n",
      "Iteration 27 cost:3.4244391925484576\n",
      "Iteration 28 cost:3.550323115232465\n",
      "Iteration 29 cost:3.720368197657045\n",
      "Iteration 30 cost:3.881143861727628\n",
      "Iteration 31 cost:3.9951077018654857\n",
      "Iteration 32 cost:4.042578191986412\n",
      "Iteration 33 cost:4.019424199907209\n",
      "Iteration 34 cost:3.932950510296032\n",
      "Iteration 35 cost:3.79805066960649\n",
      "Iteration 36 cost:3.6345089211744304\n",
      "Iteration 37 cost:3.4646114733343403\n",
      "Iteration 38 cost:3.3097832053222973\n",
      "Iteration 39 cost:3.1862695215824712\n",
      "Iteration 40 cost:3.1013787185969175\n",
      "Iteration 41 cost:3.052280229912448\n",
      "Iteration 42 cost:3.028465036116183\n",
      "Iteration 43 cost:3.0168671565442473\n",
      "Iteration 44 cost:3.0068559398181858\n",
      "Iteration 45 cost:2.992712816556727\n",
      "Iteration 46 cost:2.9732886866740973\n",
      "Iteration 47 cost:2.950220536049181\n",
      "Iteration 48 cost:2.9262955925858223\n",
      "Iteration 49 cost:2.9045556762788314\n",
      "Iteration 50 cost:2.887678601782853\n",
      "Iteration 51 cost:2.877155804263404\n",
      "Iteration 52 cost:2.8725318543643326\n",
      "Iteration 53 cost:2.871358253444855\n",
      "Iteration 54 cost:2.8701256566361657\n",
      "Iteration 55 cost:2.8657404692374975\n",
      "Iteration 56 cost:2.8567300249570855\n",
      "Iteration 57 cost:2.8435741464154938\n",
      "Iteration 58 cost:2.8281445900902025\n",
      "Iteration 59 cost:2.8127191964294367\n",
      "Iteration 60 cost:2.7991362008908043\n",
      "Iteration 61 cost:2.7884014143234612\n",
      "Iteration 62 cost:2.780703417723488\n",
      "Iteration 63 cost:2.7756038330272177\n",
      "Iteration 64 cost:2.7722422152743444\n",
      "Iteration 65 cost:2.7695658384495196\n",
      "Iteration 66 cost:2.766644216995864\n",
      "Iteration 67 cost:2.7630131835467537\n",
      "Iteration 68 cost:2.7588638445820513\n",
      "Iteration 69 cost:2.754920381117139\n",
      "Iteration 70 cost:2.752045391709644\n",
      "Iteration 71 cost:2.750799936210571\n",
      "Iteration 72 cost:2.7512001354273954\n",
      "Iteration 73 cost:2.75275791434264\n",
      "Iteration 74 cost:2.754717310100383\n",
      "Iteration 75 cost:2.7563301853313886\n",
      "Iteration 76 cost:2.757061163732758\n",
      "Iteration 77 cost:2.756687129112305\n",
      "Iteration 78 cost:2.755298821584325\n",
      "Iteration 79 cost:2.7532238115109235\n",
      "Iteration 80 cost:2.750898262877411\n",
      "Iteration 81 cost:2.7487266911063504\n",
      "Iteration 82 cost:2.746972760088694\n",
      "Iteration 83 cost:2.7457117882132493\n",
      "Iteration 84 cost:2.7448526927080863\n",
      "Iteration 85 cost:2.744214188098259\n",
      "Iteration 86 cost:2.743622793051018\n",
      "Iteration 87 cost:2.7429919319340748\n",
      "Iteration 88 cost:2.742348269030451\n",
      "Iteration 89 cost:2.741796891128148\n",
      "Iteration 90 cost:2.7414505309492054\n",
      "Iteration 91 cost:2.7413663222165545\n",
      "Iteration 92 cost:2.7415215274268827\n",
      "Iteration 93 cost:2.7418287984009044\n",
      "Iteration 94 cost:2.74216971410156\n",
      "Iteration 95 cost:2.742427563146824\n",
      "Iteration 96 cost:2.7425147366998592\n",
      "Iteration 97 cost:2.742395554053869\n",
      "Iteration 98 cost:2.7420978312708093\n",
      "Iteration 99 cost:2.741701788779404\n",
      "Iteration 100 cost:2.7413058548095197\n",
      "Iteration 101 cost:2.7409871265573917\n",
      "Iteration 102 cost:2.7407794084087476\n",
      "Iteration 103 cost:2.7406769173047434\n",
      "Iteration 104 cost:2.740652231473213\n",
      "Iteration 105 cost:2.740672076228474\n",
      "Iteration 106 cost:2.740705164445317\n",
      "Iteration 107 cost:2.7407268549492283\n",
      "Iteration 108 cost:2.740724395990884\n",
      "Iteration 109 cost:2.7406993638789863\n",
      "Iteration 110 cost:2.7406627660322185\n",
      "Iteration 111 cost:2.740625236359042\n",
      "Iteration 112 cost:2.740590312414409\n",
      "Iteration 113 cost:2.7405555386291875\n",
      "Iteration 114 cost:2.740518160451715\n",
      "Iteration 115 cost:2.7404788215695866\n",
      "Iteration 116 cost:2.7404406782921544\n",
      "Iteration 117 cost:2.7404066878464963\n",
      "Iteration 118 cost:2.740378442984808\n",
      "Iteration 119 cost:2.7403566960066463\n",
      "Iteration 120 cost:2.7403416695293727\n",
      "Iteration 121 cost:2.740332474670361\n",
      "Iteration 122 cost:2.7403268467430784\n",
      "Iteration 123 cost:2.7403221632980364\n",
      "Iteration 124 cost:2.740316975977639\n",
      "Iteration 125 cost:2.740311573080793\n",
      "Iteration 126 cost:2.7403071718394942\n",
      "Iteration 127 cost:2.7403046701376126\n",
      "Iteration 128 cost:2.740303999067709\n",
      "Iteration 129 cost:2.7403042836908105\n",
      "Iteration 130 cost:2.7403043764316375\n",
      "Iteration 131 cost:2.7403033292144228\n",
      "Iteration 132 cost:2.7403006405494277\n",
      "Iteration 133 cost:2.7402962855879336\n",
      "Iteration 134 cost:2.7402906075030273\n",
      "Iteration 135 cost:2.740284171982341\n",
      "Iteration 136 cost:2.740277630672974\n",
      "Iteration 137 cost:2.740271567199854\n",
      "Iteration 138 cost:2.7402663450899674\n",
      "Iteration 139 cost:2.7402620785962224\n",
      "Iteration 140 cost:2.740258784946031\n",
      "Iteration 141 cost:2.740256557542228\n",
      "Iteration 142 cost:2.7402555348574538\n",
      "Iteration 143 cost:2.7402556824078186\n",
      "Iteration 144 cost:2.740256656515654\n",
      "Iteration 145 cost:2.7402579271033467\n",
      "Iteration 146 cost:2.7402590228985058\n",
      "Iteration 147 cost:2.7402596533715697\n",
      "Iteration 148 cost:2.7402596727062063\n",
      "Iteration 149 cost:2.740259045694193\n",
      "Iteration 150 cost:2.7402578896448917\n",
      "Iteration 151 cost:2.740256479882232\n",
      "Iteration 152 cost:2.74025512651491\n",
      "Iteration 153 cost:2.7402540183552815\n",
      "Iteration 154 cost:2.740253189831191\n",
      "Iteration 155 cost:2.7402526105694083\n",
      "Iteration 156 cost:2.740252259084527\n",
      "Iteration 157 cost:2.7402521059117846\n",
      "Iteration 158 cost:2.7402520752162265\n",
      "Iteration 159 cost:2.740252066654557\n",
      "Iteration 160 cost:2.740252012961255\n",
      "Iteration 161 cost:2.7402518992128657\n",
      "Iteration 162 cost:2.7402517350625457\n",
      "Iteration 163 cost:2.740251533052746\n",
      "Iteration 164 cost:2.74025131681332\n",
      "Iteration 165 cost:2.7402511263989284\n",
      "Iteration 166 cost:2.7402509949588105\n",
      "Iteration 167 cost:2.7402509203451957\n",
      "Iteration 168 cost:2.7402508669574845\n",
      "Iteration 169 cost:2.740250795272417\n",
      "Iteration 170 cost:2.7402506879656787\n",
      "Iteration 171 cost:2.7402505544919347\n",
      "Iteration 172 cost:2.7402504210965435\n",
      "Iteration 173 cost:2.740250318058053\n",
      "Iteration 174 cost:2.740250267488704\n",
      "Iteration 175 cost:2.7402502730973533\n",
      "Iteration 176 cost:2.7402503172310544\n",
      "Iteration 177 cost:2.740250368768917\n",
      "Iteration 178 cost:2.7402503979703265\n",
      "Iteration 179 cost:2.740250389679496\n",
      "Iteration 180 cost:2.7402503477629354\n",
      "Iteration 181 cost:2.740250289013564\n",
      "Iteration 182 cost:2.7402502312563373\n",
      "Iteration 183 cost:2.7402501841093847\n",
      "Iteration 184 cost:2.74025014773098\n",
      "Iteration 185 cost:2.7402501174581326\n",
      "Iteration 186 cost:2.7402500887659063\n",
      "Iteration 187 cost:2.740250060056571\n",
      "Iteration 188 cost:2.740250033929827\n",
      "Iteration 189 cost:2.740250016470781\n",
      "Iteration 190 cost:2.7402500132763405\n",
      "Iteration 191 cost:2.740250024379627\n",
      "Iteration 192 cost:2.740250043025849\n",
      "Iteration 193 cost:2.740250059815043\n",
      "Iteration 194 cost:2.7402500680418833\n",
      "Iteration 195 cost:2.740250065813029\n",
      "Iteration 196 cost:2.7402500550727558\n",
      "Iteration 197 cost:2.7402500399987026\n",
      "Iteration 198 cost:2.740250025431787\n",
      "Iteration 199 cost:2.7402500148055506\n",
      "Iteration 200 cost:2.740250008575312\n"
     ]
    }
   ],
   "source": [
    "Y, mu = mean_normalization(Y)\n",
    "X, W, b = gradient_descent(1, Y, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "da661330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0683186])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = 4\n",
    "movie = 2\n",
    "cf_func(X1[movie], W1[user], b1[0, user]) + mu[movie]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
