{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4b16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Normalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca85831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_environment(name):\n",
    "    env = gym.make(name)\n",
    "    spec = gym.spec(name)\n",
    "    print(f'Action space: {env.action_space}')\n",
    "    print(f'Observation space: {env.observation_space}')\n",
    "    print(f'Max Episode steps: {spec.max_episode_steps}')\n",
    "    print(f'Nondeterministic: {spec.nondeterministic}')\n",
    "    print(f'Reward range: {env.reward_range}')\n",
    "    print(f'Reward threshold: {spec.reward_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef9f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n",
      "Max Episode steps: 1000\n",
      "Nondeterministic: False\n",
      "Reward range: (-inf, inf)\n",
      "Reward threshold: 200\n"
     ]
    }
   ],
   "source": [
    "query_environment('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39a2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a721d011",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video videos/NotWise.mp4.\n",
      "Moviepy - Writing video videos/NotWise.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/NotWise.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "vid = VideoRecorder(env, 'videos/NotWise.mp4', enabled=True)\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    vid.capture_frame()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated: break\n",
    "vid.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c78f78",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86dfa920",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000\n",
    "GAMMA = 0.995\n",
    "ALPHA = 1e-3\n",
    "MINI_BATCH_SIZE = 64\n",
    "NUM_OF_UPDATE_STEP = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb74ff",
   "metadata": {},
   "source": [
    "## Defining Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e1b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 8\n",
    "num_of_actions = 4\n",
    "\n",
    "q_network = Sequential([\n",
    "    Input(shape=(num_of_features)),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_of_actions, activation='linear')\n",
    "], name='Q_Network')\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=(num_of_features)),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_of_actions, activation='linear')\n",
    "], name='target_Q_Network')\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57662620",
   "metadata": {},
   "source": [
    "## Defining loss function\n",
    "Return denoted by $Q(s, a)$ = \"Return\" if we start at state $s$ and apply action $a$ once and behave optimally after that.\n",
    "### Bellman equation\n",
    "$Q(s, a) = R(s) + \\gamma max_{a'}Q(s', a')$\n",
    "<br>\n",
    "Our goal is to train the model to estimate the value for $Q(s, a)$. As it seems we can start with a random function of $Q$ and get to the good estimate of the function. <br>\n",
    "Our target is \"target_q_network\" but this network is implemented with random weights and will predict $Q(s', a')$ (as the target) so it has to be trained. We defined \"target_q_network\" so we won't oscillate due to moving \"target\".\n",
    "\n",
    "So target network is used to predict the $a'$ via its value. Than have $target = R(s) + \\gamma max_{a'} Q(s', a')$ and so\n",
    "```python\n",
    "max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1) # Finds maximum Q values\n",
    "y_targets = rewards + (1 - dones) * gamma * max_qsa\n",
    "```\n",
    "\n",
    "calculates target values for each of the experciences the agent have had.\n",
    "<br><br>\n",
    "However q_network is the network that we try to make reach target_q_network (target_q_network will change slightly on each gradient update). So predict $Q(s, a)$ and find the squared difference to be loss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228e42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    # Find maximum Q that optimal behavior let us into after taking first action\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1) # Choose maximum Qs\n",
    "    y_targets = rewards + (1 - dones) * gamma * max_qsa # Calculate target\n",
    "    \n",
    "    q_values = q_network(states) # Find Q values using q_network \n",
    "    # Q values that our action led to\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "    \n",
    "    loss = MSE(q_values, y_targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce342e1c",
   "metadata": {},
   "source": [
    "## Soft update\n",
    "Updates target_q_network with a little fraction of q_network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82144524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update_target_network(q_network, target_q_network, TAU=1e-3):\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc3ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "        \n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "    soft_update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b145ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_epsilon(epsilon, gamma):\n",
    "    return max(0.01, gamma * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c4b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(q_values, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        return np.argmax(q_values)\n",
    "    else: return random.choice(range(num_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa7cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "takes memory_buffer, and chooses a random sample (mini batch) and convert them to tensors\n",
    "\"\"\"\n",
    "def get_expercience(memory_buffer):\n",
    "    exp = random.sample(memory_buffer, k=MINI_BATCH_SIZE)\n",
    "    \n",
    "    states = np.array(exp, dtype='object')[:, 0]\n",
    "    states = tf.convert_to_tensor([s.flatten() for s in states], dtype=tf.float32)\n",
    "    \n",
    "    actions = np.array(exp, dtype='object')[:, 1]\n",
    "    actions = tf.convert_to_tensor(np.array(actions).astype(np.int32), dtype=tf.float32)\n",
    "    \n",
    "    rewards = np.array(exp, dtype='object')[:, 2]\n",
    "    rewards = tf.convert_to_tensor(np.array(rewards).astype(np.int32), dtype=tf.float32)\n",
    "    \n",
    "    observations = np.array(exp, dtype='object')[:, 3]\n",
    "    observations = tf.convert_to_tensor([s.flatten() for s in observations], dtype=tf.float32)\n",
    "    \n",
    "    terminated = np.array(exp, dtype='object')[:, 4]\n",
    "    terminated = tf.convert_to_tensor(np.array(terminated).astype(np.int32), dtype=tf.float32)\n",
    "    \n",
    "    return (states, actions, rewards, observations, terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ef913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep\n",
    "target_q_network.set_weights(q_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83ae65a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000: mean points are 258.78828004666567"
     ]
    }
   ],
   "source": [
    "num_episodes = 2000\n",
    "num_max_steps = 1000\n",
    "epsilon = 1\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "total_point_history = []\n",
    "num_tp_av = 100 # Number of total points for averaging\n",
    "\n",
    "for eps in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.array(state).reshape(1, -1)\n",
    "    total_points = 0\n",
    "    \n",
    "    for step in range(num_max_steps):\n",
    "        q_values = q_network(state)\n",
    "        action = get_action(q_values, epsilon)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        observation = np.array(observation).reshape(1, -1)\n",
    "        memory_buffer.append((state, action, reward, observation, terminated))\n",
    "        \n",
    "        if (step + 1) % NUM_OF_UPDATE_STEP == 0 and len(memory_buffer) >= MINI_BATCH_SIZE:\n",
    "            exp = get_expercience(memory_buffer)\n",
    "            agent_learn(exp, GAMMA)\n",
    "        \n",
    "        state = observation.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if terminated: break\n",
    "            \n",
    "    epsilon = get_new_epsilon(epsilon, GAMMA)\n",
    "    total_point_history.append(total_points)\n",
    "    points_mean = np.mean(total_point_history[-num_tp_av:])\n",
    "    \n",
    "    print(f'\\rEpisode {eps + 1}: mean points are {points_mean}', end=\"\")\n",
    "    \n",
    "    if points_mean >= 300.0:\n",
    "        print(f'Solved in {eps + 1} episodes')\n",
    "        q_network.save('LunarLander.h5')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69215675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video videos/Wise.mp4.\n",
      "Moviepy - Writing video videos/Wise.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/Wise.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "vid = VideoRecorder(env, 'videos/Wise.mp4', enabled=True)\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    vid.capture_frame()\n",
    "    action = get_action(q_network(np.array(state).reshape(1, -1)), 0)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated: break\n",
    "    state = observation.copy()\n",
    "vid.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
